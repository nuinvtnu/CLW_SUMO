{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bdIE2jvbBoV4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NJI8pwl7ucJ"},"outputs":[],"source":["pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48pz2CHV78h5"},"outputs":[],"source":["pip install -U gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BY8i4APY51wG"},"outputs":[],"source":["# Load library\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, matthews_corrcoef, roc_curve, auc\n","import pickle\n","from keras.models import load_model\n","import pandas as pd\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import pickle as pk\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","\n","#from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","\n","#from keras.utils.np_utils import to_categorical\n","import re\n","from __future__ import print_function\n","from keras.models import Sequential\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import model_from_json\n","from keras import backend as K\n","from __future__ import print_function\n","import _pickle as cPickle\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","\n","from keras.layers import LSTM, GRU, SimpleRNN\n","from keras.layers import Convolution1D, MaxPooling1D\n","\n","from keras.callbacks import EarlyStopping,ModelCheckpoint\n","from sklearn.model_selection import StratifiedKFold\n","from keras.models import model_from_json\n","from keras.models import load_model\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","\n","import random\n","from gensim.models import Word2Vec\n","from gensim.models.word2vec import LineSentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-3UIRNd8M-j"},"outputs":[],"source":["def twoTupleDic():\n","    AA_list_sort = ['G','A','V','L','I','M','P','F','W','S','T','N','Q','Y','C','K','R','H','D','E','X']\n","\n","    AA_dict = {}\n","    numm = 1\n","    for i in AA_list_sort:\n","        for j in AA_list_sort:\n","          for jj in AA_list_sort:\n","\n","            AA_dict[i+j+jj] = numm\n","            numm += 1\n","    return AA_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssybE7NBVXJt"},"outputs":[],"source":["def ProSentence(pro, K):\n","\tsentence = \"\"\n","\tlength = len(pro)\n","\tfor i in range(length - K + 1):\n","\t\tsentence += pro[i: i + K] + \" \"\n","    #delete extra space\n","\tsentence = sentence[0 : len(sentence) - 1]\n","\treturn sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rf6pznQnxpdl"},"outputs":[],"source":["path_data = \"/content/drive/MyDrive/CLW_SUMO/Data/\"\n","path_result = \"/content/drive/MyDrive/CLW_SUMO/Results/\"\n","path_model = \"/content/drive/MyDrive/CLW_SUMO/Model/\"\n"]},{"cell_type":"code","source":["resutl_file=\"WCL_Sumo.txt\"\n","name_result_test =\"Independent_test.txt\""],"metadata":{"id":"hBucL6fplqrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDBq_G51-KQo","outputId":"4d90a0b5-9407-40e1-c0e9-a097a17381f6","executionInfo":{"status":"ok","timestamp":1725967970528,"user_tz":-420,"elapsed":6,"user":{"displayName":"Xuân Trần","userId":"16348586452814696727"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[['SWS', 'WSP', 'SPK', 'PKI', 'KIK', 'IKR', 'KRE', 'REK', 'EKS', 'KSS', 'SSV'], ['HVV', 'VVF', 'VFG', 'FGQ', 'GQV', 'QVV', 'VVE', 'VEG', 'EGL', 'GLN', 'LNV']]\n"]}],"source":["# Tokenizer train data input Word2vec\n","data_token = []\n","for i in df_train['n_gram']:\n","   data_token.append(i.split())\n","print(data_token[:2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud1AQoG3ea0w"},"outputs":[],"source":["# load test data\n","file_test =\"Test_Sumo.csv\"\n","df_test =pd.read_csv(path_data+file_test)\n","text_test =[] #PTMsequend kmer\n","for i in df_test['Sequence']:\n","  temp = ProSentence(i,k) # Biểu diễn dữ liệu đầu vào thành token Kmer\n","  text_test.append(temp)\n","df_test['n_gram'] =text_test\n","df_test\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import pickle as cPickle\n","import pandas as pd\n","max_features =300\n","\n","word_index1 = twoTupleDic() # Tham chiếu texts thành word_index1  trong bộ từ điển\n","test_sequences = []\n","for each in text_test:\n","    each_index_list = []\n","    each = each.split(' ')\n","    for i in each:\n","        each_index_list.append(word_index1[i])\n","    test_sequences.append(each_index_list)\n","# Tokenizer train data input Word2vec\n","data_token = []\n","for i in df_test['k_mer']:\n","   data_token.append(i.split())\n","# Len of the K_mer[1]\n","len(data_token[1])\n","MAX_SEQUENCE_LENGTH = len(data_token[1])\n","print(MAX_SEQUENCE_LENGTH)\n","print(data_token[:2])\n","Xtest = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","ytest = np.array(df_test['Label'])"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from keras.models import load_model\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# --- Load mô hình ---\n","model = load_model(\"./CLW_SUMO.h5\")\n","# --- Dự đoán ---\n","y_probs = model.predict(Xtest)\n","y_preds = (y_probs > 0.5).astype(int).flatten()\n","y_true = ytest.flatten()\n","\n","# --- Tính các chỉ số ---\n","acc = accuracy_score(y_true, y_preds)\n","prec = precision_score(y_true, y_preds)\n","rec = recall_score(y_true, y_preds)\n","f1 = f1_score(y_true, y_preds)\n","auc_score = roc_auc_score(y_true, y_probs)\n","\n","cm = confusion_matrix(y_true, y_preds)\n","tn, fp, fn, tp = cm.ravel()\n","\n","# --- In kết quả ---\n","print(\"===== Evaluation on Independent Test Set =====\")\n","print(f\"Accuracy : {acc:.4f}\")\n","print(f\"Precision: {prec:.4f}\")\n","print(f\"Recall   : {rec:.4f}\")\n","print(f\"F1-Score : {f1:.4f}\")\n","print(f\"AUC-ROC  : {auc_score:.4f}\")\n","print(f\"Confusion Matrix: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n","\n","# --- Lưu kết quả ra Excel (tùy chọn) ---\n","df_metrics = pd.DataFrame([{\n","    \"Accuracy\": acc,\n","    \"Precision\": prec,\n","    \"Recall\": rec,\n","    \"F1-score\": f1,\n","    \"AUC-ROC\": auc_score,\n","    \"TP\": tp,\n","    \"FP\": fp,\n","    \"TN\": tn,\n","    \"FN\": fn\n","}])\n","\n","df_metrics.to_excel(\"CLW_SUMO_ID.xlsx\", index=False)\n","print(\"\\nĐã lưu kết quả vào 'CLW_SUMO_ID.xlsx'.\")\n","\n","# --- Vẽ đường cong ROC ---\n","fpr, tpr, _ = roc_curve(y_true, y_probs)\n","plt.figure(figsize=(6, 5))\n"],"metadata":{"id":"xc0RkKkHpuFI"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}